# Overhuman

**AI-ассистент который учится и становится дешевле с каждым запросом.**

Overhuman — это daemon-процесс, который принимает задачи из любого канала (CLI, HTTP API, Telegram, Slack, Discord, Email), выполняет их через LLM, запоминает результаты и автоматически генерирует код для повторяющихся задач. Со временем дорогие LLM-вызовы заменяются детерминированным кодом — быстрее, дешевле, надёжнее.

```
Задачи → LLM выполняет → Рефлексия → Паттерны → Код-навыки
  ↑                                                    ↓
  └──── Дешевле, быстрее, надёжнее ◄──── Код заменяет LLM
```

## Что умеет

**Универсальный ассистент** — помогает не только с кодом. Маркетинг, планирование, анализ данных, коммуникации, автоматизация рутины.

**Помнит всё** — короткая память (текущий диалог) + долгосрочная (SQLite с полнотекстовым поиском) + отслеживание паттернов. Не нужно повторяться.

**Учится на повторениях** — если ты просишь одно и то же 3 раза, Overhuman автоматически генерирует код-навык и больше не тратит деньги на LLM для этой задачи.

**Работает с любой моделью** — OpenAI, Claude, Ollama (локальные модели), LM Studio, Groq, Together AI, OpenRouter, или любой OpenAI-совместимый endpoint. Переключается одной командой.

**Многоканальный** — один мозг за всеми каналами. Задача приходит из Telegram, ответ уходит в Slack. CLI для разработки, HTTP API для интеграций, Email для автоматизации.

**Всегда включён** — работает как системный сервис. Heartbeat каждые 30 минут для проактивных задач и саморазвития.

**Безопасный** — шифрование ключей (AES-256-GCM), защита от prompt injection, аудит всех действий, валидация навыков, sandbox для кода.

## Быстрый старт

```bash
# Собрать
go build -o overhuman ./cmd/overhuman/

# Настроить (интерактивный визард — выбор провайдера, ввод API ключа)
./overhuman configure

# Запустить в режиме чата
./overhuman cli

# Или как daemon с HTTP API
./overhuman start

# Проверить что всё ОК
./overhuman doctor
```

При первом запуске `overhuman cli` автоматически предложит пройти настройку если ключи не заданы.

## Поддерживаемые LLM

| Провайдер | API ключ | Особенности |
|-----------|----------|-------------|
| **OpenAI** | Нужен | GPT-4o, GPT-4o-mini |
| **Anthropic Claude** | Нужен | Claude Sonnet, Haiku, Opus |
| **Ollama** | Не нужен | Локальные модели — llama3, mistral, и др. Бесплатно |
| **LM Studio** | Не нужен | Локальные модели через GUI |
| **Groq** | Нужен | Быстрый inference — Llama, Mixtral |
| **Together AI** | Нужен | Open-source модели в облаке |
| **OpenRouter** | Нужен | Доступ ко всем моделям через один ключ |
| **Custom** | Опционально | Любой OpenAI-совместимый сервер |

Конфиг хранится в `~/.overhuman/config.json` (права 600). Env-переменные перекрывают конфиг — удобно для Docker/CI.

## Как это работает

### 10-стадийный pipeline

Каждый запрос проходит через полный цикл обработки:

1. **Приём** — нормализация входа из любого канала в единый формат
2. **Уточнение** — LLM задаёт уточняющие вопросы если нужно
3. **Планирование** — декомпозиция задачи в подзадачи (DAG)
4. **Выбор агента** — подбор специализированного субагента
5. **Выполнение** — параллельное выполнение подзадач
6. **Ревью** — обязательная проверка качества результата
7. **Память** — сохранение в короткую и долгосрочную память
8. **Паттерны** — отслеживание повторяющихся задач
9. **Рефлексия** — самооценка и корректировка стратегий
10. **Цели** — обновление проактивных целей

### Самообучение

Overhuman отслеживает повторяющиеся задачи через fingerprinting. Когда паттерн повторяется K раз (по умолчанию 3), система:

1. Генерирует код-навык на основе накопленных примеров
2. Регистрирует его как детерминированную альтернативу LLM-вызову
3. При следующем повторении использует код вместо LLM
4. Если код ломается — автоматический откат на LLM

Результат: каждый цикл делает систему дешевле (код вместо API), быстрее (мс вместо секунд) и надёжнее (детерминизм вместо стохастики).

### 4 уровня рефлексии

| Уровень | Когда | Что делает |
|---------|-------|-----------|
| **Микро** | Каждый шаг pipeline | Корректирует следующий шаг |
| **Мезо** | После каждой задачи | Обновляет память, навыки, паттерны |
| **Макро** | Каждые N задач | Пересматривает стратегии и цели |
| **Мега** | Редко | Оценивает сам процесс рефлексии |

### Фрактальные агенты

Агенты образуют дерево. Родитель создаёт специализированных детей (кодер, ревьюер, исследователь), делегирует задачи, устраивает соревнования (best-of-N) и увольняет/продвигает по результатам. У каждого агента своя идентичность, память и навыки.

## HTTP API

```bash
# Асинхронный запрос (fire-and-forget)
curl -X POST http://localhost:9090/input \
  -H "Content-Type: application/json" \
  -d '{"payload": "Проанализируй этот CSV файл", "sender": "user1"}'

# Синхронный запрос (ждёт ответа)
curl -X POST http://localhost:9090/input/sync \
  -H "Content-Type: application/json" \
  -d '{"payload": "Переведи на английский: Привет мир"}'

# Проверка здоровья
curl http://localhost:9090/health
```

## Конфигурация

Env-переменные (перекрывают config.json):

```
ANTHROPIC_API_KEY   — ключ Claude
OPENAI_API_KEY      — ключ OpenAI
LLM_PROVIDER        — провайдер: openai, claude, ollama, groq, together, openrouter, custom
LLM_API_KEY         — ключ для любого провайдера
LLM_MODEL           — модель по умолчанию
LLM_BASE_URL        — URL для custom/ollama
OVERHUMAN_DATA      — директория данных (по умолчанию ~/.overhuman)
OVERHUMAN_API_ADDR  — адрес API (по умолчанию 127.0.0.1:9090)
OVERHUMAN_NAME      — имя агента
```

## Технические решения

| Решение | Выбор | Почему |
|---------|-------|--------|
| Язык | **Go** | Daemon-first, goroutines, single binary 15MB, <10MB RAM |
| Хранение | **SQLite + файлы** | Self-contained, человекочитаемо, FTS5 для поиска |
| Зависимости | **3 штуки** | `google/uuid`, `modernc.org/sqlite`, `golang.org/x/term` |
| Инструменты | **MCP** | Стандарт индустрии (Anthropic + OpenAI + Google + Microsoft) |
| Sandbox | **Docker** | Изоляция для автогенерированного кода |
| Шифрование | **AES-256-GCM** | Аутентифицированное шифрование ключей |

## Структура проекта

```
cmd/overhuman/       — точка входа (daemon, CLI, configure, doctor)
internal/
├── soul/            — идентичность агента (markdown DNA, версионирование)
├── agent/           — фрактальная иерархия агентов
├── pipeline/        — 10-стадийный оркестратор + DAG executor
├── brain/           — LLM интеграция, роутинг моделей, сборка контекста
├── senses/          — входные каналы (CLI, HTTP, Telegram, Slack, Discord, Email)
├── instruments/     — система навыков (LLM/Code/Hybrid), генератор кода, Docker sandbox
├── memory/          — короткая + долгосрочная память + паттерны + общая база знаний
├── reflection/      — 4 уровня рефлексии
├── evolution/       — fitness-метрики, A/B тестирование, выбраковка навыков
├── goals/           — проактивный движок целей
├── budget/          — контроль расходов, лимиты, роутинг по бюджету
├── versioning/      — версионирование с автооткатом при деградации
├── security/        — санитизация, аудит, шифрование, валидация
├── mcp/             — MCP клиент и реестр (JSON-RPC 2.0)
├── storage/         — персистентное KV-хранилище (SQLite, FTS5, TTL)
├── skills/          — 20 стартовых навыков
└── observability/   — структурированные логи и метрики
```

## Тесты

```bash
go test ./...         # 586 тестов, 19 пакетов
go test ./... -race   # Проверка на race conditions
```

Все тесты работают с mock LLM сервером — API ключи для запуска не нужны.

## Документация

- `docs/SPEC.md` — полная спецификация (700+ строк)
- `docs/PHASES.md` — трекер реализации
- `docs/ARCHITECTURE.md` — архитектура

## Лицензия

MIT
